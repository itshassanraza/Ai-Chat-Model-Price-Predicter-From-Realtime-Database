import firebase_admin
from firebase_admin import credentials, firestore
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import joblib
import logging
from datetime import datetime
import re
from flask import Flask, request, jsonify
from pyngrok import ngrok

# Initialize Firebase app only if it is not already initialized
if not firebase_admin._apps:
    cred = credentials.Certificate('/content/ai-munshi-firebase-adminsdk-anf5i-1ef1fc2c8b.json')  # Provide the correct path
    firebase_admin.initialize_app(cred)

# Access Firestore
db = firestore.client()

# Reference to a collection (Firestore collections are like Realtime Database nodes)
collection_ref = db.collection('items')  # Assume 'items' is the collection name

# Fetch data from Firestore
docs = collection_ref.stream()

data_list = []

for doc in docs:
    data = doc.to_dict()
    data_list.append(data)

data = pd.DataFrame(data_list)

print(f"Columns in the fetched data: {data.columns.tolist()}")

columns_to_drop = ['uid', 'type', 'document id', 'name', 'date']
columns_to_drop_existing = [col for col in columns_to_drop if col in data.columns]

CleanData = data.drop(columns_to_drop_existing, axis=1)

if 'amount' in CleanData.columns and 'quantity' in CleanData.columns:
    CleanData['UnitPrice'] = CleanData['amount'] / CleanData['quantity']
    CleanData = CleanData.drop(['quantity', 'amount'], axis=1)

# Label encode the 'itemName' column
if 'itemName' in CleanData.columns:
    label_encoder = LabelEncoder()
    CleanData['itemName'] = label_encoder.fit_transform(CleanData['itemName'])

X = CleanData.drop(['UnitPrice'], axis=1, errors='ignore')
Y = CleanData['UnitPrice'] if 'UnitPrice' in CleanData.columns else None

if Y is not None:
    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    predictions = model.predict(X_test)
    mse = mean_squared_error(y_test, predictions)
    r2 = r2_score(y_test, predictions)
    print(f'Mean Squared Error: {mse}')
    print(f'R-Squared: {r2}')

    joblib.dump(model, 'AiMunshi.joblib')
    joblib.dump(scaler, 'Scalar.pkl')

else:
    logging.warning("Target column 'UnitPrice' is missing or empty. Model cannot be trained.")

app = Flask(__name__)

item_code_mapping = {
    'apples': 0, 'bananas': 1, 'beef': 2, 'chicken': 3,
    'cooking oil': 4, 'eggs': 5, 'ghee': 6, 'lentils': 7,
    'milk': 8, 'mutton': 9, 'onions': 10, 'petrol': 11,
    'potatoes': 12, 'rice': 13, 'shampoo': 14, 'soap': 15,
    'sugar': 16, 'tea': 17, 'tomatoes': 18, 'wheat flour': 19,
}

try:
    model = joblib.load('AiMunshi.joblib')
    scaler = joblib.load('Scalar.pkl')
    logging.info("Model and scaler loaded successfully.")
except Exception as e:
    logging.error(f"Error loading model or scaler: {str(e)}")
    raise

@app.route('/predict', methods=['POST'])
def predict():
    try:
        data = request.json
        logging.info(f"Request received: {data}")

        query_text = data.get('queryResult', {}).get('queryText', '')
        item_name, date = None, None

        for item in item_code_mapping:
            if item in query_text.lower():
                item_name = item
                break

        date_match = re.search(r'\b(\d{1,2}/\d{1,2}/\d{2,4})\b', query_text)
        if date_match:
            date_str = date_match.group(1)
            try:
                date = datetime.strptime(date_str, '%d/%m/%Y')
            except ValueError:
                try:
                    date = datetime.strptime(date_str, '%d/%m/%y')
                except ValueError:
                    date = None

        if not item_name or not date:
            logging.warning("Item or date not found. Please provide correct information.")
            return jsonify({"fulfillmentText": "Sorry, unable to process your request."})

        item_code = item_code_mapping.get(item_name)

        model_input = [[date.month, date.day, date.year, item_code]]

        scaled_input = scaler.transform(model_input)

        predicted_price = model.predict(scaled_input)

        formatted_date = date.strftime('%d/%m/%Y')
        rounded_price = round(predicted_price[0], 1)

        response_text = f"The predicted price of {item_name} on {formatted_date} is PKR {rounded_price}."
        logging.info(response_text)

        return jsonify({"fulfillmentText": response_text})

    except Exception as e:
        logging.error(f"Unexpected error: {str(e)}")
        return jsonify({"fulfillmentText": f"An error occurred: {str(e)}"})

if __name__ == '__main__':
    try:
        public_url = ngrok.connect(8000)
        logging.info(f"ngrok tunnel created: {public_url}")
        print(f" * ngrok tunnel \"{public_url}\" -> \"http://127.0.0.1:8000\"")
    except Exception as e:
        logging.error(f"Error starting ngrok tunnel: {str(e)}")
        raise

    app.run(host='127.0.0.1', port=8000)
